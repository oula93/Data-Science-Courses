{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d55c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2.feature_column as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75b709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d57c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Networks\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e182505",
   "metadata": {},
   "source": [
    "<big><b>1. Creating Tensors</b></big>\n",
    "\n",
    "Types of tensors:\n",
    "- Variable\n",
    "- Constant\n",
    "- Placeholder\n",
    "- SparseTensor\n",
    "\n",
    "all types except for Variable are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b1a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Variable('this is a string',tf.string)\n",
    "n = tf.Variable(12, tf.int16)\n",
    "x = tf.Variable([[34.5,12.0],[0.0, 11.9]], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e521de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a7112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones((1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad5408",
   "metadata": {},
   "source": [
    "<b><big>2. Tensor Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249a59ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank/Degree of Tensors (dim)\n",
    "tf.rank(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca3f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c315c60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of a tensor\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9878c754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping a tensor\n",
    "y = tf.Variable([1,2,3,4,5,6], tf.float64)\n",
    "\n",
    "tf.reshape(y,(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eca9b1",
   "metadata": {},
   "source": [
    "<b><big>3. Linear Regression</b></big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e48ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
    "x_test = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
    "\n",
    "y_train = x_train.pop('survived')\n",
    "y_test = x_test.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cce27129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1  female  38.0                   1      0  71.2833  First        C   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5350c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f2a7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Categorical data\n",
    "categorical = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
    "numeric = ['age','fare']\n",
    "\n",
    "#create an object to map out categorical data into integers\n",
    "features = []\n",
    "for col in categorical:\n",
    "    vocab = x_train[col].unique()\n",
    "    features.append(tf.feature_column.categorical_column_with_vocabulary_list(col,vocab))\n",
    "    \n",
    "for col in numeric:\n",
    "    features.append(tf.feature_column.numeric_column(col,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76386730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='n_siblings_spouses', vocabulary_list=(1, 0, 3, 4, 2, 5, 8), dtype=tf.int64, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='parch', vocabulary_list=(0, 1, 2, 5, 3, 4), dtype=tf.int64, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='class', vocabulary_list=('Third', 'First', 'Second'), dtype=tf.string, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='deck', vocabulary_list=('unknown', 'C', 'G', 'A', 'B', 'D', 'F', 'E'), dtype=tf.string, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Southampton', 'Cherbourg', 'Queenstown', 'unknown'), dtype=tf.string, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "VocabularyListCategoricalColumn(key='alone', vocabulary_list=('n', 'y'), dtype=tf.string, default_value=-1, num_oov_buckets=0) \n",
      "\n",
      "NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None) \n",
      "\n",
      "NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    print(col,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d2cf1",
   "metadata": {},
   "source": [
    "<b>3.1 Input function</b>\n",
    "- When training the model, data is fed into small batches of 32 rows.\n",
    "- We'll be feeding the data again and again in different orders. Each stream of the entire data is called an <b>epoch.</b>\n",
    "- We use multiple epochs to make the model see the data in different angles and give better predictions.\n",
    "- Too many epochs can lead to overfitting.\n",
    "\n",
    "- The tensorflow model to be used requires that the data we pass comes as a tf.data.Dataset object.\n",
    "- An <b>input function</b> will convert our current dataframe into that object\n",
    "- An input function defines how our dataset will be converted to batches at each Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56192f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(x, y, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function():  # inner function, this will be returned\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(x), y))  # create tf.data.Dataset object with data and its label\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(1000)  # randomize order of data\n",
    "        dataset = dataset.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
    "        return dataset  # return a batch of the dataset\n",
    "    return input_function  # return a function object for use\n",
    "\n",
    "#our input functions\n",
    "train_input_fn = make_input_fn(x_train, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "test_input_fn = make_input_fn(x_test, y_test, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99bb5d",
   "metadata": {},
   "source": [
    "<b>3.2 Building the Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31d4bfbb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\OULA~1.AAB\\\\AppData\\\\Local\\\\Temp\\\\tmplc5rbnbi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#we create a linear estimator by passing the feature columns we created earlier\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991c313",
   "metadata": {},
   "source": [
    "<b>3.3 Training & Evaluating the Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40889dd9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt-200\n",
      "WARNING:tensorflow:From C:\\Users\\oula.aabkari\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 200...\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 200...\n",
      "INFO:tensorflow:loss = 0.48511228, step = 200\n",
      "INFO:tensorflow:global_step/sec: 362.354\n",
      "INFO:tensorflow:loss = 0.4519024, step = 300 (0.277 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 400...\n",
      "INFO:tensorflow:Saving checkpoints for 400 into C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 400...\n",
      "INFO:tensorflow:Loss for final step: 0.41865543.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-10-19T16:29:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.57596s\n",
      "INFO:tensorflow:Finished evaluation at 2021-10-19-16:29:51\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.7765151, accuracy_baseline = 0.625, auc = 0.8188858, auc_precision_recall = 0.7726903, average_loss = 0.50317013, global_step = 400, label/mean = 0.375, loss = 0.49204478, precision = 0.74390244, prediction/mean = 0.31626442, recall = 0.61616164\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt-400\n"
     ]
    }
   ],
   "source": [
    "linear_est.train(train_input_fn) #train\n",
    "evaluation_params = linear_est.evaluate(test_input_fn) #evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f0eeafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7765151,\n",
       " 'accuracy_baseline': 0.625,\n",
       " 'auc': 0.8188858,\n",
       " 'auc_precision_recall': 0.7726903,\n",
       " 'average_loss': 0.50317013,\n",
       " 'label/mean': 0.375,\n",
       " 'loss': 0.49204478,\n",
       " 'precision': 0.74390244,\n",
       " 'prediction/mean': 0.31626442,\n",
       " 'recall': 0.61616164,\n",
       " 'global_step': 400}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fa464",
   "metadata": {},
   "source": [
    "<b>3.4 Predicting</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82a22d3f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmplc5rbnbi\\model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(linear_est.predict(test_input_fn)) #we get a generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d5cab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                          male\n",
      "age                          35.0\n",
      "n_siblings_spouses              0\n",
      "parch                           0\n",
      "fare                         8.05\n",
      "class                       Third\n",
      "deck                      unknown\n",
      "embark_town           Southampton\n",
      "alone                           y\n",
      "Name: 0, dtype: object\n",
      "actual result:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': array([-2.793911], dtype=float32),\n",
       " 'logistic': array([0.05765414], dtype=float32),\n",
       " 'probabilities': array([0.9423459 , 0.05765411], dtype=float32),\n",
       " 'class_ids': array([0], dtype=int64),\n",
       " 'classes': array([b'0'], dtype=object),\n",
       " 'all_class_ids': array([0, 1]),\n",
       " 'all_classes': array([b'0', b'1'], dtype=object)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.loc[0])\n",
    "print('actual result: ',y_test[0]) \n",
    "y_pred[0] #dictionary for prediction of the 1st person\n",
    "#probabilities:[%survived=0  %survived=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4bf33",
   "metadata": {},
   "source": [
    "<b><big>4. Classification</b></big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ea3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "species = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fca675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = tf.keras.utils.get_file(\"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "x_train = pd.read_csv(train_path, names=col_names, header=0)\n",
    "x_test = pd.read_csv(test_path, names=col_names, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e5965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train.pop('Species')\n",
    "y_test = x_test.pop('Species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d642e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a084a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3769bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "#Feature columns:\n",
    "features = []\n",
    "for col in x_train.columns:\n",
    "    features.append(tf.feature_column.numeric_column(col,dtype=tf.float32))\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbda8c",
   "metadata": {},
   "source": [
    "<b>4.1 Input Function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0b7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(x,y,shuffle=True,batch_size=256): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(x), y)) \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000).repeat()  \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8afc0b",
   "metadata": {},
   "source": [
    "<b>4.2 Building the Model</b>\n",
    "\n",
    "- DNNClassifier (Deep Neural Networks)\n",
    "- LinearClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4dd1c40",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\OULA~1.AAB\\\\AppData\\\\Local\\\\Temp\\\\tmpxko2mjhh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=features,\n",
    "                                        hidden_units=[30,10],\n",
    "                                        n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e430b",
   "metadata": {},
   "source": [
    "<b>4.3 Training & Evaluating the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d49a35",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oula.aabkari\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\oula.aabkari\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 1.1736454, step = 0\n",
      "INFO:tensorflow:global_step/sec: 507.63\n",
      "INFO:tensorflow:loss = 0.80583143, step = 100 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.004\n",
      "INFO:tensorflow:loss = 0.6948378, step = 200 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 679.025\n",
      "INFO:tensorflow:loss = 0.6356728, step = 300 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 651.869\n",
      "INFO:tensorflow:loss = 0.6231326, step = 400 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.008\n",
      "INFO:tensorflow:loss = 0.5958797, step = 500 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.316\n",
      "INFO:tensorflow:loss = 0.5732478, step = 600 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 662.812\n",
      "INFO:tensorflow:loss = 0.547006, step = 700 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 661.266\n",
      "INFO:tensorflow:loss = 0.5345686, step = 800 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.317\n",
      "INFO:tensorflow:loss = 0.5250528, step = 900 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 646.793\n",
      "INFO:tensorflow:loss = 0.5161116, step = 1000 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 647.691\n",
      "INFO:tensorflow:loss = 0.51834655, step = 1100 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 661.685\n",
      "INFO:tensorflow:loss = 0.50752777, step = 1200 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 688.545\n",
      "INFO:tensorflow:loss = 0.5017948, step = 1300 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 658.894\n",
      "INFO:tensorflow:loss = 0.5009955, step = 1400 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 641.652\n",
      "INFO:tensorflow:loss = 0.47809875, step = 1500 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 678.899\n",
      "INFO:tensorflow:loss = 0.47361004, step = 1600 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.904\n",
      "INFO:tensorflow:loss = 0.47169462, step = 1700 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.732\n",
      "INFO:tensorflow:loss = 0.45899412, step = 1800 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.502\n",
      "INFO:tensorflow:loss = 0.44265848, step = 1900 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.082\n",
      "INFO:tensorflow:loss = 0.45011064, step = 2000 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.294\n",
      "INFO:tensorflow:loss = 0.44473028, step = 2100 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.592\n",
      "INFO:tensorflow:loss = 0.4283721, step = 2200 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 635.197\n",
      "INFO:tensorflow:loss = 0.43320024, step = 2300 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 651.507\n",
      "INFO:tensorflow:loss = 0.42548913, step = 2400 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.005\n",
      "INFO:tensorflow:loss = 0.40441978, step = 2500 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 691.907\n",
      "INFO:tensorflow:loss = 0.41410512, step = 2600 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 647.734\n",
      "INFO:tensorflow:loss = 0.4074226, step = 2700 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.339\n",
      "INFO:tensorflow:loss = 0.4204319, step = 2800 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.208\n",
      "INFO:tensorflow:loss = 0.4011409, step = 2900 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.615\n",
      "INFO:tensorflow:loss = 0.39475188, step = 3000 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 660.35\n",
      "INFO:tensorflow:loss = 0.40312344, step = 3100 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.304\n",
      "INFO:tensorflow:loss = 0.38518178, step = 3200 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 638.898\n",
      "INFO:tensorflow:loss = 0.38068742, step = 3300 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.595\n",
      "INFO:tensorflow:loss = 0.393259, step = 3400 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.319\n",
      "INFO:tensorflow:loss = 0.36821997, step = 3500 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.171\n",
      "INFO:tensorflow:loss = 0.37464577, step = 3600 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.289\n",
      "INFO:tensorflow:loss = 0.3785425, step = 3700 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 679.438\n",
      "INFO:tensorflow:loss = 0.36756626, step = 3800 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 642.345\n",
      "INFO:tensorflow:loss = 0.36813137, step = 3900 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.32\n",
      "INFO:tensorflow:loss = 0.35695434, step = 4000 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 641.262\n",
      "INFO:tensorflow:loss = 0.35051158, step = 4100 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 646.313\n",
      "INFO:tensorflow:loss = 0.36310476, step = 4200 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.299\n",
      "INFO:tensorflow:loss = 0.35712892, step = 4300 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 678.529\n",
      "INFO:tensorflow:loss = 0.35058123, step = 4400 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.49\n",
      "INFO:tensorflow:loss = 0.34466416, step = 4500 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 642.744\n",
      "INFO:tensorflow:loss = 0.3505153, step = 4600 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.506\n",
      "INFO:tensorflow:loss = 0.33540976, step = 4700 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 660.903\n",
      "INFO:tensorflow:loss = 0.33996075, step = 4800 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 663.586\n",
      "INFO:tensorflow:loss = 0.3285924, step = 4900 (0.150 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
      "INFO:tensorflow:Loss for final step: 0.33913043.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-10-20T10:13:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.39469s\n",
      "INFO:tensorflow:Finished evaluation at 2021-10-20-10:13:27\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.93333334, average_loss = 0.3914405, global_step = 5000, loss = 0.3914405\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\\model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "classifier.train(input_fn=lambda: input_fn(x_train,y_train),steps=5000)\n",
    "\n",
    "evaluation_params = classifier.evaluate(input_fn=lambda: input_fn(x_test,y_test,shuffle=False)) #evaluate\n",
    "#we use lambda to avoid creating an innder function like previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7abbc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93333334,\n",
       " 'average_loss': 0.3914405,\n",
       " 'loss': 0.3914405,\n",
       " 'global_step': 5000}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194494ce",
   "metadata": {},
   "source": [
    "<b>4.4 Prediction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e314137d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\OULA~1.AAB\\AppData\\Local\\Temp\\tmpxko2mjhh\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(classifier.predict(input_fn=lambda: input_fn(x_test,y_test,shuffle=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0681e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLength    5.9\n",
      "SepalWidth     3.0\n",
      "PetalLength    4.2\n",
      "PetalWidth     1.5\n",
      "Name: 0, dtype: float64\n",
      "actual result:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': array([-0.6912698,  1.9586961,  1.6368738], dtype=float32),\n",
       " 'probabilities': array([0.03935081, 0.5569539 , 0.40369523], dtype=float32),\n",
       " 'class_ids': array([1], dtype=int64),\n",
       " 'classes': array([b'1'], dtype=object),\n",
       " 'all_class_ids': array([0, 1, 2]),\n",
       " 'all_classes': array([b'0', b'1', b'2'], dtype=object)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.loc[0])\n",
    "print('actual result: ',y_test[0])\n",
    "y_pred[0] # 56% that it's class 1: correct prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891644a",
   "metadata": {},
   "source": [
    "<b><big>５. Hidden Markov Models</b></big>\n",
    "\n",
    "A hidden markov model works with probabilities to predict future events or states.\n",
    "https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1ef67",
   "metadata": {},
   "source": [
    "<b>Data:</b> For a markov model we are only interested in probability distributions that have to do with states.\n",
    "\n",
    "<b>Components of a Markov Model:</b>\n",
    "\n",
    "- States: In each markov model we have a finite set of states. These states could be something like \"warm\" and \"cold\" or \"high\" and \"low\" or even \"red\", \"green\" and \"blue\". These states are \"hidden\" within the model, which means we do not direcly observe them.\n",
    "\n",
    "- Observations: Each state has a particular outcome or observation associated with it based on a probability distribution. An example of this is the following: On a hot day Tim has a 80% chance of being happy and a 20% chance of being sad.\n",
    "\n",
    "- Transitions: Each state will have a probability defining the likelyhood of transitioning to a different state. An example is the following: a cold day has a 30% chance of being followed by a hot day and a 70% chance of being follwed by another cold day.\n",
    "\n",
    "To create a hidden markov model we need: \n",
    "- States\n",
    "- Observation Distribution\n",
    "- Transition Distribution\n",
    "\n",
    "For our purpose we will assume we already have this information available as we attempt to predict the weather on a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de3c5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e11e51e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_probability==0.14.1\n",
      "  Downloading tensorflow_probability-0.14.1-py2.py3-none-any.whl (5.7 MB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "Collecting cloudpickle>=1.3\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\oula.aabkari\\anaconda3\\lib\\site-packages (from tensorflow_probability==0.14.1) (5.0.6)\n",
      "Requirement already satisfied: absl-py in c:\\users\\oula.aabkari\\anaconda3\\lib\\site-packages (from tensorflow_probability==0.14.1) (0.14.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\oula.aabkari\\anaconda3\\lib\\site-packages (from tensorflow_probability==0.14.1) (0.4.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\oula.aabkari\\anaconda3\\lib\\site-packages (from tensorflow_probability==0.14.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\oula.aabkari\\anaconda3\\lib\\site-packages (from tensorflow_probability==0.14.1) (1.19.5)\n",
      "Installing collected packages: dm-tree, cloudpickle, tensorflow-probability\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.1.1\n",
      "    Uninstalling cloudpickle-1.1.1:\n",
      "      Successfully uninstalled cloudpickle-1.1.1\n",
      "  Attempting uninstall: tensorflow-probability\n",
      "    Found existing installation: tensorflow-probability 0.8.0rc0\n",
      "    Uninstalling tensorflow-probability-0.8.0rc0:\n",
      "      Successfully uninstalled tensorflow-probability-0.8.0rc0\n",
      "Successfully installed cloudpickle-2.0.0 dm-tree-0.1.6 tensorflow-probability-0.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.2.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_probability==0.14.1 --user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6e2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc9a3c",
   "metadata": {},
   "source": [
    "<b>Weather Model</b><br>\n",
    "Taken direclty from the TensorFlow documentation (https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel).\n",
    "\n",
    "We will model a simple weather system and try to predict the temperature on each day given the following information.\n",
    "\n",
    "1. Cold days are encoded by a 0 and hot days are encoded by a 1.\n",
    "2. The first day in our sequence has an 80% chance of being cold.\n",
    "3. A cold day has a 30% chance of being followed by a hot day.\n",
    "4. A hot day has a 20% chance of being followed by a cold day.\n",
    "5. On each day the temperature is normally distributed with mean and standard deviation 0 and 5 on a cold day and mean and standard deviation 15 and 10 on a hot day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0942ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions  # making a shortcut for later on\n",
    "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
    "transition_distribution = tfd.Categorical(probs=[[0.5, 0.5],\n",
    "                                                 [0.2, 0.8]])  # refer to points 3 and 4 above\n",
    "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
    "\n",
    "# the loc argument represents the mean and the scale is the standard devitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c558eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfd.HiddenMarkovModel(initial_distribution=initial_distribution,\n",
    "                              transition_distribution=transition_distribution,\n",
    "                              observation_distribution=observation_distribution,\n",
    "                              num_steps=7) #num_steps = num days we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355358ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
       "array([11.999999, 11.1     , 10.830001, 10.748999, 10.724698, 10.71741 ,\n",
       "       10.715222], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the expected temperatures\n",
    "mean = model.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe506c61",
   "metadata": {},
   "source": [
    "<b><big>５. Neural Networks</b></big>\n",
    "\n",
    "<b>5.1 Data Preprocessing</b>\n",
    "\n",
    "For this tutorial we will use the MNIST Fashion Dataset. This is a dataset that is included in keras.\n",
    "\n",
    "This dataset includes 60,000 images for training and 10,000 images for validation/testing.\n",
    "\n",
    "We're trying to predict clothing items: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e214bfb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "40960/29515 [=========================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "26435584/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist \n",
    "\n",
    "#split the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bff9093f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have 60,000 images that are made up of 28x28 pixels \n",
    "#pixel values are between 0 and 255\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ca818eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290f8a6dbb0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example of 1 item\n",
    "plt.imshow(train_images[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9437b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are integers ranging from 0 - 9\n",
    "np.unique(train_labels)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2089d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "#we do a simple preprocessing by scaling all our greyscale pixel values (0-255) to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d7719",
   "metadata": {},
   "source": [
    "<b>5.2 Building the Model</b>\n",
    "\n",
    "We are going to use a keras sequential model with three different layers. This model represents a feed-forward neural network (one that passes values from left to right).\n",
    "\n",
    "- <b>Input layer</b>: Flatten means that we reshape each image (28,28) into 784 pixels and each pixel will be associayed with 1 neuron. \n",
    "- <b>Hidden layer</b>: Dense denotes that this hidden layer will be fully connected; each neuron from the previous layer connects to each neuron from this layer. It has 128 neurons and uses the rectify linear unit activation function.\n",
    "- <b>Output layer</b>: Another dense layer which has 10 neuros. Each neuron represnts the probabillity of a given image being one of the 10 different classes. The activation function softmax is used on this layer to calculate a probabillity distribution for each class.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82c11bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),  # input layer (1)\n",
    "                          keras.layers.Dense(128, activation='relu'),  # hidden layer (2)\n",
    "                          keras.layers.Dense(10, activation='softmax')]) # output layer (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff4968",
   "metadata": {},
   "source": [
    "<b>5.3 Compiling the Model</b>\n",
    "\n",
    "It's to define the loss function, optimizer and metrics we would like to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7927590",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc36d78",
   "metadata": {},
   "source": [
    "<b>5.4 Training & Evaluating the Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cb83a2e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4960 - accuracy: 0.8269\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3746 - accuracy: 0.8652\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3357 - accuracy: 0.8773\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3146 - accuracy: 0.8841\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2943 - accuracy: 0.8920\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2813 - accuracy: 0.8959\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2700 - accuracy: 0.8999\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2598 - accuracy: 0.9038\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2484 - accuracy: 0.9073\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2382 - accuracy: 0.9122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290f86901c0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1293c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 815us/step - loss: 0.3487 - accuracy: 0.8817\n",
      "Test accuracy: 0.8816999793052673\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,test_labels,verbose=1) \n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a584291",
   "metadata": {},
   "source": [
    "<b>5.5 Making Predictions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e56a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55a653",
   "metadata": {},
   "source": [
    "If we wan't to get the value with the highest score we can use a useful function from numpy called argmax(). This simply returns the index of the maximium value from a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c93ec94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  9 =  Ankle boot\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290f36f4be0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_0 = np.argmax(predictions[0])\n",
    "print('prediction: ',prediction_0,'= ',class_names[prediction_0])\n",
    "print('')\n",
    "plt.imshow(train_images[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20320feb",
   "metadata": {},
   "source": [
    "<b><big>6. Convolutional Neural Networks</b></big>\n",
    "\n",
    "The fundemental difference between a dense layer and a convolutional layer is that dense layers detect patterns globally while convolutional layers detect patterns locally. When we have a densly connected layer each node in that layer sees all the data from the previous layer. This means that this layer is looking at all the information and is only capable of analyzing the data in a global capacity. Our convolutional layer however will not be densly connected, this means it can detect local patterns using part of the input data to that layer.\n",
    "\n",
    "Assume that the goal of our network will be to determine whether an image is a cat or not\n",
    "\n",
    "<b>Dense Layer:</b> A dense layer will consider the ENTIRE image. It will look at all the pixels and use that information to generate some output.\n",
    "\n",
    "A dense neural network learns patterns that are present in one specific area of an image. This means if a pattern that the network knows is present in a different area of the image it will have to learn the pattern again in that new area to be able to detect it.\n",
    "\n",
    "Let's consider our dense neural network that has learned what an eye looks like from a sample of cat images. Let's say it's determined that an image is likely to be a cat if an eye is present in a certain location\n",
    "\n",
    "Since our densly connected network has only recognized patterns globally it will look where it thinks the eyes should be present.If the image is flipped, it would likely determine this image is not a cat. Even though the pattern of the eyes is present, it's just in a different location.\n",
    "\n",
    "<b>Convolutional Layer:</b> The convolutional layer will look at specific parts of the image and detect patterns.\n",
    "\n",
    "Convolutional layers learn and detect patterns from different areas of the image. They know what an eye looks like and by analyzing different parts of the image can find where it is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c5f13",
   "metadata": {},
   "source": [
    "<b>6.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c2603",
   "metadata": {},
   "source": [
    "The problem we will consider here is classifying 10 different everyday objects. The dataset is built into tensorflow and called the CIFAR Image Dataset. It contains 60,000 32x32 color images with 6000 images of each class.\n",
    "\n",
    "Labels: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1dbe63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 31s 0us/step\n",
      "170508288/170498071 [==============================] - 31s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#loading\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c132f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4776609f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290f37d1130>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdpklEQVR4nO2dWYzk13Xev1Nrr7P09Cw9i2aG1IQJtZCiGzQtSjRpygYtKJAYQIL1IPBB8PjBAiLEeSDoIFLypDiRZMFOFIwsxrShyFIiEaITJRFDJCBsKTSH23DIocRt9qVn6Z7eaz156GIwou53utlL9UT3+wGNrr6n7/9/6lad+lfdr8455u4QQvzyU1hvB4QQ3UHBLkQmKNiFyAQFuxCZoGAXIhMU7EJkQmklk83sPgBfA1AE8Gfu/qXo/zds3ORbt40QK5cAzdKvSYWC0TkevI5FYqOBH9PIRD5jkbNZ5P+yjgijUmpwruCAoTAb3/F3frI1YLXPFru/vLOxWfGp0tbLY2cxNTmefGSWHexmVgTwbwH8JoDTAJ42s8fc/WU2Z+u2EXzpjx9O2trtNj1Xb7WaHK/09NA57WJ6DgA0nb8QlFCktmIrPV7mrofPDi9xPxrslQXxk6DQIlYv0znNBj9iq0DuNLCsYI++1xF+5yM4V7sd+E8mhi+mgR/R87TVCtYqOh8Zb4ZrlfbjX/6TT9E5K3kbfzuA19z9DXevA/grAB9fwfGEEGvISoJ9F4BT1/x9ujMmhLgOWUmwp94f/cL7DjM7aGaHzezw5NXxFZxOCLESVhLspwHsuebv3QDOvv2f3P2Qu4+6++iGjZtXcDohxEpYSbA/DeCAme03swqA3wHw2Oq4JYRYbZa9G+/uTTP7HID/gQXp7WF3f2mxeW2yq1qq8t3ieju9yzlzdYrOKffz7dtiuZfa4Hxem+zsNoOd89Z8g9rmr85RW6WHqwkt8B3h6bnp5HjB+PEG+jdSmwfnage7z0ZkxeXuggdLHO7Gs8cs2viPdtwjH6PdeLYeANAmq9JepirAWJHO7u4/BPDDlRxDCNEd9A06ITJBwS5EJijYhcgEBbsQmaBgFyITVrQb/05ptVuYnElLQ40Gl6guXbycHD99ZozOKfb0U9vAIP9yT7XAJSqmytWb3Pd2o0lts1PptQCA3jL3AwUuu0zV03Jkvc6lnxv2H6C2d9+4l9p6o0QkIg2FklGQ7OKBsR3pciwvaLkJOcskkt4K5L61A9lzOejKLkQmKNiFyAQFuxCZoGAXIhMU7EJkQld346dnZvDj//MTYuM70wWkk2TmanzXdL6V3sEHgHKF24pt/vrXIhuq88533FvBTnF/he9m9xp/aHqqvHRWq1BPjs/McMXg8JHnqG3s0i9kLf8/bti/n9qGh4eT4719fXSOR+WlgiSTNinRBADGHs9u18KLkmtY0tAyEmGiObqyC5EJCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhO6mwjTamNiOl13zYPab0ayGUoVXreuL5CuigVuq6BCbfNIyz/N4DVzanaG2uZmuK1qXF4bcJ4kUyR3rVzldffmp+ep7fVTZ6jtxLnz1LZpQ7qu3Z7du+mcrcNb+PE28+SlUiHo4kNkueUmu7CGOwCvd7fY+Vh3l7gG3Tv3X1d2ITJBwS5EJijYhcgEBbsQmaBgFyITFOxCZMKKpDczOw5gCkALQNPdR6P/b7tjrp6WGcrlyBWSFdTimVwObrNi0KYnUDTqjbRE1QhcH+wboLapyVlqm6zz1lC1IIOqUklLh4MVfseKRS43zjRrfF6QIVi7dDU5PjHBsxv7B7g8ODKyk9pu3H8DtQ1U0jJllawTENdDbARl4RxcAowy85gsF6mDTAKMavWths5+j7tfWoXjCCHWEL2NFyITVhrsDuBHZvaMmR1cDYeEEGvDSt/G3+nuZ81sG4DHzewVd3/y2n/ovAgcBICe/g0rPJ0QYrms6Mru7mc7v8cAPArg9sT/HHL3UXcfrfQEfdGFEGvKsoPdzPrNbPCt2wB+C8DR1XJMCLG6rORt/HYAj3ba2pQA/Ed3/+/RhLY75mpp+arW4K87rHVOT9B+KMoJChLswlZCzDYTFMvs6eUnq5aDwpENPm++xmW5ppEsr+B+VYKssfhywI9ZKqWPGfkxNcvX8eqrx6jt0mUuBg32pLPvdu/i2Xebgwy7SpA9GPWvajd5UdImUeWibMqWp+XjNZHe3P0NALcsd74QortIehMiExTsQmSCgl2ITFCwC5EJCnYhMqGrBSfdHXWS/WMtnhXE+lq1C4GGFlENCgMW+etfu5CWT0rBKjaC7LVKiUuHA708K2u2zgtENpH2MWiLh1qTG6tBcc5ikOXl5DrSaAcSFCnoCQCFAn9czl8Zo7aztXRfv9dOnKRztm5N96kDgJ0791DbwMAgtfVUA5mYSJ8ND6Q30vuuFRSi1JVdiExQsAuRCQp2ITJBwS5EJijYhciE7u7GA2gGtbgYLbKDOz89ReeUgi3yVrCJXyrUqY0l0JTLUfJBsMRBLbmoGN5A0PaqSV6+g3JxaAR+NFt8PQrGD+oku6MV7Li3ilHRNW6KarWZpdeqGRSTmzw7Tm0nzh2ntmqF77j39fVRG0voiurklcvp+1Wv8bqGurILkQkKdiEyQcEuRCYo2IXIBAW7EJmgYBciE7qeCFNrpKUcVmcOANrky/2sbQ4ANIM6bXOBPFEOZK0ikZqqJT7HSU04ADAP2gUFcpi3uQ7F8iBmWzwBpQ5+rkJQn64ePGZlolN6gZ+rUeD3K5LXCsWghp6lk4aCvJqwfmE70DDrc7yG3uRMoB0yebPGj8fiZW52ks7RlV2ITFCwC5EJCnYhMkHBLkQmKNiFyAQFuxCZsKj0ZmYPA/gYgDF3f29nbAjAdwDsA3AcwKfcnacKdWi325idT0shpUgLaRM3A3lqbuYCtVUqXFwZ2s7bAvUS9aQQyFrFoJacFxrUdnU8XTsNAOamubyyd/9NyfGpRj+dMz5+ldqqVZ6t1SAyKgAYSVNrRxoaX8ZwXis4ZAXpNS4Ug1p4QeutVpQ+GGUB1maorT1xKjl++cwb/FykPl0jkP+WcmX/cwD3vW3sQQBPuPsBAE90/hZCXMcsGuydfutX3jb8cQCPdG4/AuATq+uWEGK1We5n9u3ufg4AOr+3rZ5LQoi1YM2/LmtmBwEcBIBSD//cKIRYW5Z7Zb9gZiMA0PlNq/S7+yF3H3X30WKluszTCSFWynKD/TEAD3RuPwDgB6vjjhBirViK9PZtAHcDGDaz0wC+AOBLAL5rZp8FcBLAJ5dyMoej1SSSRyCfbK72Jsc39HNZaK4vuGvGJaPyNM+W6yHVHLdt41sW8728CGG9yaW33h5+34p96fUAgL4NG5Ljm/pH6JwdwzVqi7Lv5gM5bJbMO3+RS6KNmQlqKztfq1KTt8MqttOPdaMRFCst8rVvgz+e7aBVFub4+SbPHk+O18b5Wk1Ppx+zJin0CSwh2N3908R072JzhRDXD/oGnRCZoGAXIhMU7EJkgoJdiExQsAuRCV0tOAl3oJmWQjb2DdJpm4iMdubcSTpnLvgCTy3IUrPzJ6ht/5a0xLZtzy4655WzZ6nN2zy7qm+GS4Ab+7n88+KpF5LjAzt41tVAlRfMfPNnL1Nbq38ztW068P70uXa+m86ZOXGM2opBpt8G55les9MT6fEp+j0wVMoD1DY5z4tb9m7aSm1bevljPU0y8xD0JDSWJRoUONWVXYhMULALkQkKdiEyQcEuRCYo2IXIBAW7EJnQdemt0ErLDDsGuNxxYTwtkzQGuTZRGuRSXsG4fNJs8LqZe297T3J8POiVVt8cZK8ZX/7CBi6vTUzyDKqp+bRk156doHNq81yK3Bj4cWqaS14zF9MFM/du2kTn7LwpLdcBwMTLPLNt5gyXS8cvpG2TM7ygZ4tkNwLA1Tn+nOvdzKW3wT3c1iT92ebneDYi68FngV6nK7sQmaBgFyITFOxCZIKCXYhMULALkQld3Y0vFYsY2pDeJR8e4LvnE1fStbiGengCR7XMdyWbDb77vO3GdPskALhhZE9y/KWTvE3Ppipv/9QM2idt27GJ2grDXLmYKaVfvwuD3I/xi+epbe823g5rtsL9H2+lE2+ujF+kcwoj76K23TffQW1nTr9CbfNzs8nxcpE/PzzoJ1Vs81p4tQmeXHMRXEFpzqZ9LBT5tbhFWpFF6MouRCYo2IXIBAW7EJmgYBciExTsQmSCgl2ITFhK+6eHAXwMwJi7v7cz9kUAvwvgLR3lIXf/4WLHqpSL2LtjKGn7R7/9G3TeiTf2Jcen5nkiRm2ey0LNGpfe9u3k8o+305KMD++gc64G8trMLPd/9zBvKdV0nngzPZNOGPEeXpNvwHktuWKbazzbN/I2VDNjaYlt+kxaZgKARo3fr/7tXALc+Z4PU1u7cTU5Pnb2dTpndprLZAjWY0M/T7AqgdcUdBKFjVl+LicJLx605FrKlf3PAdyXGP+qu9/a+Vk00IUQ68uiwe7uTwK40gVfhBBryEo+s3/OzI6Y2cNmxt8HCiGuC5Yb7F8HcCOAWwGcA/Bl9o9mdtDMDpvZ4RoprCCEWHuWFezufsHdW+7eBvANALcH/3vI3UfdfbTawzd0hBBry7KC3cxGrvnzfgBHV8cdIcRasRTp7dsA7gYwbGanAXwBwN1mdisAB3AcwO8t5WRFc2wopqWhX7uNS163vyfdXmlqltfoajh/HWs0uTzRnOUfNebm0+fbX+ftn2ZrXD6ZDlo8lcv8oRmf5K2Qevans9vmanytfNMwtZ05f47aXn2Tt9+6eXNaOjx5MdjrbXPpqtXDsyIH9t5GbR++cV9y/MopLr399NlnqG3s/E+prd94/ULUePut+RapJ9fmUmSpnJ5TJzUegSUEu7t/OjH8zcXmCSGuL/QNOiEyQcEuRCYo2IXIBAW7EJmgYBciE7pacLLdbGL6SlqeOP0ml+p379qfHN81sp3OKfVxqaYdtF2avHSJ2iYm0r5vGdpC58zMcSlkdi7IiJvmUs3U9EZqu+nGG9LHmwmknzkuAW7t5dly5Rq/b7/yqx9Mjl+Z5XOOn09nqAFAvcDbULXmeGsokJZMO9+ffk4BwNb3/ya1NcfTxU8B4Mqxp6jtzaNPU9ul13+WHC9U+GNWKKVlOQuKqerKLkQmKNiFyAQFuxCZoGAXIhMU7EJkgoJdiEzoqvRWLBSxqbc/aZu6zPuNnSPZP8M7eL+ujUV+1/oHN1EbNnLJrmhp2WgwSNPfGPSw88Ly+sAde5n3Ntu6NS019fXxrMLZQOa7ZR/P6Pv1UZ5tNkcyC2e5MoQDe3iG4IXLXB48e55n0p1/81Ry/GTQz20+kG17N/HCl5vemyrVuMCtN/0ate1680hy/MiPeWnHi+ffTI678YKeurILkQkKdiEyQcEuRCYo2IXIBAW7EJnQ1d34crGIkaF0EofVeYLElQtjyfEXjrxG5zx3lNcK275rD7V9+NfvorZdW9O+z4/zHdBiKdiqD3bjSyX+0LxrJy/T39tTTo5XK/x1fUOlj9owyH1stLgfUyQBaK7FFZRjrx6ntvFaup0UANx2Q1qBAIDpbel1fPMcV3+OneBqxwtv8OfcVHUTtQ1v4Gt88/a04jF6F0/Iee4njyfHT7wWJM9QixDilwoFuxCZoGAXIhMU7EJkgoJdiExQsAuRCebOEwIAwMz2APgLADsAtAEccvevmdkQgO8A2IeFFlCfcveg/w2weXDA7x59X9L2vnel2wUBwMYtaWnlmZe4RPJKIOPcec+91NYEX49/eO+HkuObe/icnl6eVFEqczlmbp7LeVu38LXqq6YTjepB+6cIKwZttIJrhZXTNeNePXGazvmjf/1Vars0xpNdfvWO9OMCAB/75GeS417jdeuOPv131Ha2yaXDlyZ4u6Z2kdfy87mJ5PiBICbOvPpscvzHTzyGq1cuJZ1cypW9CeAP3P0fALgDwO+b2c0AHgTwhLsfAPBE528hxHXKosHu7ufc/dnO7SkAxwDsAvBxAI90/u0RAJ9YIx+FEKvAO/rMbmb7AHwAwFMAtrv7OWDhBQEAf88hhFh3lhzsZjYA4HsAPu/uvGfwL847aGaHzexwrcG/EiuEWFuWFOxmVsZCoH/L3b/fGb5gZiMd+wiA5BfY3f2Qu4+6+2i1nP7ethBi7Vk02M3MsNCP/Zi7f+Ua02MAHujcfgDAD1bfPSHEarGUrLc7AXwGwItm9nxn7CEAXwLwXTP7LICTAD652IEarTYuTqQlpVfKPKupOHY5OX7y3Dk6565776a2h/7ZH1Lbn/zpv6O2//rXjyXH//4u3v6pXClSW//gBmprtXg9tqGNQ9S2dSjdEivKoqtUeGZbIWiVNd3iBeXqpfR15Ov//j/QOS+/8iK1Vcvcx0cf+0/UtvsmIvUe+Ht0Tm+Vt5ra4Pw+7xygJjTJegDADMkE9DqXS/fuStcUPBys06LB7u5/A4CJi1ywFkJcV+gbdEJkgoJdiExQsAuRCQp2ITJBwS5EJnS14GSlWsWufe9O2lqYovMajXSGUqWfax0je3jbIjeepbZnJ2/v8z9/8L3k+NR5Xnixr5dnO1V7g2KUVAABqiX+5aSBvvSa9PXyDLtKINf0VLiP3sPv28W59OP50rGX6ZyPfISLO7fcegu1fePPuJz3kyf/W3L8hh2b6JxKH5dLL53nhSpfePVn1Fbu5+u4fUPal9Ycl197SQFR/qzRlV2IbFCwC5EJCnYhMkHBLkQmKNiFyAQFuxCZ0FXpzeFoIi0ntNpcDqtU07JRP08aw+Q0L9h4YYxn2F26wmtmnj6fzr7zJi/K0VPlkkujwaWVqAxotcwftv5qWpYrlric1NvDs7x6erhk1y5yoefkxQtpg/M5n7j/fmr74Ac/SG2nTvEilo8+9tfJ8ede2EvntObr1DZ+4Sq11S+fobZSixcenW1OJ8ffGD9F5/RV03JprTZH5+jKLkQmKNiFyAQFuxCZoGAXIhMU7EJkQld345vNFi5NpHe0G03ejqdUSL8meZPvZj935Ci1ve+WXwnm8TporN1RvcR33OsNvgt+7twlapsP2hNVgnpyZXK6KEGiXOGJNeVg57/lvN3R9Hx6V3hoOF0jDwCGt/BaflOTvHr5jpEd1HZlPK28/OhHP6Rz5qdnqO3y5fTOOQDMGL92loKEqCJRKDZvT7c9A4Bt29P3uRnULtSVXYhMULALkQkKdiEyQcEuRCYo2IXIBAW7EJmwqPRmZnsA/AWAHQDaAA65+9fM7IsAfhfAW9rGQ+7O9Qws1H5rWVqusSKvgzY9m05qmZvmMsj5i2mJDwD++E/+lNpOvHaC+1FPyxqvneGJNR4k+EQtnhotLmtZi7cFKpLXbwvENwtqnbnxdkeRnAdP3+/efu775cv8MasGLaomr3JZrlZL+3/8OE+esUDSbfCHBR4kDUWJTawGYH+V11icnUn72A6eb0vR2ZsA/sDdnzWzQQDPmNnjHdtX3f3fLOEYQoh1Zim93s4BONe5PWVmxwDw0q1CiOuSd/SZ3cz2AfgAgKc6Q58zsyNm9rCZ8XrKQoh1Z8nBbmYDAL4H4PPuPgng6wBuBHArFq78XybzDprZYTM73KzzIg9CiLVlScFuZmUsBPq33P37AODuF9y95e5tAN8AcHtqrrsfcvdRdx8tBd/BFkKsLYsGu5kZgG8COObuX7lmfOSaf7sfAM88EUKsO0vZjb8TwGcAvGhmz3fGHgLwaTO7FQuqwnEAv7foyUolDG0ZIlaeHTZHspBqQfunQpCBNDE+QW1btm6jto1D6SykZiB3tJ3XM2s2uAzVanLJK6pd126kfYlkvlqN+9gmEhoAIMh6K5DryESQvfa3P/5barvnnnuo7aWXj1Ebu9v14DErBs/FdvC8iuTSVi34CFtP+3LqBK9BV6yma9o1go/KS9mN/xukJdVQUxdCXF/oG3RCZIKCXYhMULALkQkKdiEyQcEuRCaYR9LKKrNxaKN/6N4PJW3tIJuIdIxCMRATSkFRRovucpDxxDKKCkUu1TTrvA1Vu8Ulr1Yg47SDxWIPZ7PBpbzpGZ49WKtxebDRCPwn6xgdr6+XF+7ct38/tR1+5llqm5hMF+6MsgCjmGgFtqCzFWBhjmCSQoE/r3r60hl289MTaLWayZPpyi5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhM6GqvN4PBLC0nlMv8dceKRLZocTmjXA5y56NErkAiqTKJLZhTCVbY0ENtkVTWinRKIg1F8uCWYZaJCDQCPzzIemPSYbvNpc2ZGS5Tnr9wgdr27eOy3NRMOgtsdi7di24B/gRphrJcIIkGjxl7bAqkx+GCLf2cG5uf4nOoRQjxS4WCXYhMULALkQkKdiEyQcEuRCYo2IXIhK5Kbw6De1pm8HbQi4xkKEWJRFFmWCjLlbhEZeSEhciR4HjFQFopBwURGw1eVJAWlgxcjPrRFY2vVbPFZTmm9JWD+9w7uInadr2L93qL+pvNkf58kaQYPXesyP2PsuWiYxbJYsVFQtPZg1evXKJzdGUXIhMU7EJkgoJdiExQsAuRCQp2ITJh0d14M+sB8CSAauf//7O7f8HMhgB8B8A+LLR/+pS7j0fH8rajPp/eYWQ73QDANkCjnd1w9zOqTxfsnjtJkGgHiRMWtAsqBDvd5V5u8yLfja8Gu8Wc5dVja0Ytqurp+nTtIFkkOt5sPUq64bvW8830WkXPN7DEKwAenCtKdqlUuJoQ1Utk9JEadGHyzBKOWwPwG+5+CxbaM99nZncAeBDAE+5+AMATnb+FENcpiwa7L/BW+dFy58cBfBzAI53xRwB8Yi0cFEKsDkvtz17sdHAdA/C4uz8FYLu7nwOAzm/e/lQIse4sKdjdveXutwLYDeB2M3vvUk9gZgfN7LCZHWaf44QQa8872s1x9wkA/xvAfQAumNkIAHR+j5E5h9x91N1Hy8EmhRBibVk02M1sq5lt6tzuBfARAK8AeAzAA51/ewDAD9bIRyHEKrCUPf8RAI/YQvG4AoDvuvt/MbOfAPiumX0WwEkAn1zKCZ32yOFyB2slBOMySLVapbY4kYTbypW0HBbJfCVwCa0VJGM0ozp5UcIFkQFZzTIglqEsStapBkk+5fS7uOhckYQWrXGDyGsAUGin17gdnKsZ2IpBj6d2IB1Gj9lyWrBxiY37t2iwu/sRAB9IjF8GcO9SnRNCrC/6Bp0QmaBgFyITFOxCZIKCXYhMULALkQm2nG3/ZZ/M7CKAE50/hwHwglndQ378PPLj5/n/zY+97r41ZehqsP/cic0Ou/voupxcfsiPDP3Q23ghMkHBLkQmrGewH1rHc1+L/Ph55MfP80vjx7p9ZhdCdBe9jRciE9Yl2M3sPjP7qZm9ZmbrVrvOzI6b2Ytm9ryZHe7ieR82szEzO3rN2JCZPW5mr3Z+b14nP75oZmc6a/K8mX20C37sMbP/ZWbHzOwlM/vHnfGurkngR1fXxMx6zOzvzOyFjh//ojO+svVw967+ACgCeB3ADQAqAF4AcHO3/ej4chzA8Dqc9y4AtwE4es3YHwF4sHP7QQD/ap38+CKAf9rl9RgBcFvn9iCAnwG4udtrEvjR1TXBQp7qQOd2GcBTAO5Y6Xqsx5X9dgCvufsb7l4H8FdYKF6ZDe7+JIArbxvuegFP4kfXcfdz7v5s5/YUgGMAdqHLaxL40VV8gVUv8roewb4LwKlr/j6NdVjQDg7gR2b2jJkdXCcf3uJ6KuD5OTM70nmbv+YfJ67FzPZhoX7CuhY1fZsfQJfXZC2KvK5HsKdKaayXJHCnu98G4LcB/L6Z3bVOflxPfB3AjVjoEXAOwJe7dWIzGwDwPQCfd/fJbp13CX50fU18BUVeGesR7KcB7Lnm790Azq6DH3D3s53fYwAexcJHjPViSQU81xp3v9B5orUBfANdWhMzK2MhwL7l7t/vDHd9TVJ+rNeadM49gXdY5JWxHsH+NIADZrbfzCoAfgcLxSu7ipn1m9ngW7cB/BaAo/GsNeW6KOD51pOpw/3owprYQmG6bwI45u5fucbU1TVhfnR7TdasyGu3dhjfttv4USzsdL4O4A/XyYcbsKAEvADgpW76AeDbWHg72MDCO53PAtiChTZar3Z+D62TH38J4EUARzpPrpEu+PEhLHyUOwLg+c7PR7u9JoEfXV0TAO8H8FznfEcB/PPO+IrWQ9+gEyIT9A06ITJBwS5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhMULALkQn/F+sAtT5Mlu3cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[4] ,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8679f",
   "metadata": {},
   "source": [
    "<b>6.2 CNN Architecture</b>\n",
    "\n",
    "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few densly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71af7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5889191",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "#Convolutional Base:\n",
    "#Layer1: The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3x3 over our input data. \n",
    "#We will also apply the activation function relu to the output of each convolution operation.\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "\n",
    "#Layer2: This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#Other layers\n",
    "#Here the frequency of filters is increased from 32 to 64. We can do this as our data shrinks in spacial dimensions as \n",
    "#it passed through the layers, meaning we can afford (computationally) to add more depth.\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa9a3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Dense Layers\n",
    "model.add(layers.Flatten())  #flatten the conv layer to turn them into an 'input' layer for the dense layers\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc381134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06728970",
   "metadata": {},
   "source": [
    "<b>6.3 Training & Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8401c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.5360 - accuracy: 0.4386 - val_loss: 1.3153 - val_accuracy: 0.5256\n",
      "Epoch 2/4\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1779 - accuracy: 0.5803 - val_loss: 1.0720 - val_accuracy: 0.6176\n",
      "Epoch 3/4\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.0310 - accuracy: 0.6343 - val_loss: 1.0233 - val_accuracy: 0.6401\n",
      "Epoch 4/4\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.9268 - accuracy: 0.6741 - val_loss: 1.0056 - val_accuracy: 0.6562\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=4, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7e1e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.0056 - accuracy: 0.6562\n",
      "0.6561999917030334\n"
     ]
    }
   ],
   "source": [
    "#Evaluating\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c140f",
   "metadata": {},
   "source": [
    "<b><big>7. Natural Language Processing</b></big>\n",
    "\n",
    "<b>7.1 Encoding Text</b><br>\n",
    "\n",
    "machine learning models and neural networks don't take raw text data as an input.\n",
    "\n",
    "a) Bag of words\n",
    "\n",
    "The first and simplest way to encode our data is to use something called <b>bag of words</b>. Each word in a sentence is encoded with an integer and thrown into a collection that does not maintain the order of the words but does keep track of the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b05ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}   #maps words to integers\n",
    "word_encoding = 1\n",
    "\n",
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "    \n",
    "    words = text.lower().split(\" \")\n",
    "    bag = {}  #stores the encodings and their frequency\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding = vocab[word]  #get encoding\n",
    "        else:\n",
    "            vocab[word] = word_encoding  #set new encoding\n",
    "            encoding = word_encoding\n",
    "            word_encoding += 1\n",
    "        \n",
    "        if encoding in bag:\n",
    "            bag[encoding] += 1  #increase the count of an existing encoding\n",
    "        else:\n",
    "            bag[encoding] = 1   #it's a new encoding, set its count to 1\n",
    "    \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f08eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: {1: 1, 2: 1, 3: 1, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1}\n",
      "Negative: {1: 1, 2: 1, 3: 1, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 13: 1, 10: 1, 11: 1, 12: 1, 9: 1}\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "print(\"Positive:\", bag_of_words(positive_review))\n",
    "print(\"Negative:\", bag_of_words(negative_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3fb827",
   "metadata": {},
   "source": [
    "b) Integer Encoding\n",
    "\n",
    "This involves representing each word or character in a sentence as a unique integer and maintaining the order of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a0378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "word_encoding = 1\n",
    "\n",
    "def one_hot_encoding(text):\n",
    "    global word_encoding\n",
    "    \n",
    "    words = text.lower().split(\" \")\n",
    "    encoding = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding.append(vocab[word])\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding.append(vocab[word])\n",
    "            word_encoding += 1\n",
    "            \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69400999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 5, 12, 13]\n",
      "Negative: [1, 2, 3, 4, 5, 6, 7, 8, 13, 10, 11, 5, 12, 9]\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "print(\"Positive:\", one_hot_encoding(positive_review))\n",
    "print(\"Negative:\", one_hot_encoding(negative_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0858ea",
   "metadata": {},
   "source": [
    "b) Word Embeddings\n",
    "\n",
    "This third method is far superior. It keeps the order of words intact as well as encodes similar words with very similar labels. It attempts to not only encode the frequency and order of words but the meaning of those words in the sentence. It encodes each word as a dense vector that represents its context in the sentence.\n",
    "\n",
    "Unlike the previous techniques word embeddings are learned by looking at many different training examples. You can add what's called an embedding layer to the beggining of your model and while your model trains your embedding layer will learn the correct embeddings for words. You can also use pretrained embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81612711",
   "metadata": {},
   "source": [
    "<b>7.2 Recurrent Neural Networks</b><br>\n",
    "\n",
    "- Up until this point we have been using <b>feed-forward</b> neural networks. all our data is fed forwards (all at once) from left to right through the network. This won't work very well for processing text. \n",
    "\n",
    "- We humans process the text by reading word by word from left to right and keeping track of the current meaning of the sentence so we can understand the meaning of the next word. a recurrent neural network is designed to do the same thing. \n",
    "\n",
    "- A <b>recurrent neural network</b> is a network that contains a loop. It will process one word at a time while maintaining an internal memory of what it's already seen. This will allow it to treat words differently based on their order in a sentence and to slowly build an understanding of the entire input, one word at a time.\n",
    "\n",
    "- We treat our text data as a sequence so that we can pass one word at a time to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b53657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b7fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8253fdf8",
   "metadata": {},
   "source": [
    "<u>Course Notebook Link:</u><br>\n",
    "- Core Learning Algorithms: https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#forceEdit=true&sandboxMode=true&scrollTo=5suI1lmskE7p <br> \n",
    "- Neural Networks with TensorFlow: <br>\n",
    "https://colab.research.google.com/drive/1m2cg3D1x3j5vrFc-Cu0gMvc48gWyCOuG#forceEdit=true&sandboxMode=true <br>\n",
    "- Deep Computer Vision (Convolutional Neural Networks): <br>\n",
    "https://colab.research.google.com/drive/1ZZXnCjFEOkp_KdNcNabd14yok0BAIuwS#forceEdit=true&sandboxMode=true\n",
    "- Natural Language Processing\n",
    "https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true\n",
    "- Reinforcement Learning\n",
    "https://colab.research.google.com/drive/1IlrlS3bB8t1Gd5Pogol4MIwUxlAjhWOQ#forceEdit=true&sandboxMode=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
